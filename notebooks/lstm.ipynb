{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import dagshub\n",
    "import mlflow\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # GPU is available\n",
    "    print(\"GPU is available.\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    # GPU is not available\n",
    "    print(\"GPU is not available. PyTorch will use the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagshub.init(repo_owner=\"stephenjera\", repo_name=\"Genre-Classification\", mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "DURATION = 30  # length of audio files measured in seconds\n",
    "NUM_SEGMENTS = 1\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = Path.cwd().parent / \"data\"\n",
    "genres_dir = data_directory / \"genres\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mfcc(\n",
    "    dataset_path: str | Path,\n",
    "    json_path: str,\n",
    "    samples_per_track: int,\n",
    "    n_mfcc=13,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    num_segments=5,\n",
    "):\n",
    "    \"\"\"Creates a JSON file of the MFCCs for the dataset\n",
    "    :param\n",
    "    ----------\n",
    "     dataset_path: path to the dataset folder\n",
    "     json_path: name of JSON file to be created\n",
    "     n_mfcc: number of MFCC coefficients to create\n",
    "     n_fft:\n",
    "     hop_length:\n",
    "     num_segments:\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a dictionary to map semantic labels to numerical labels\n",
    "    semantic_to_numeric = {}\n",
    "    # dictionary to store data\n",
    "    data = {\n",
    "        \"mappings\": {},\n",
    "        \"mfcc\": [],\n",
    "        \"labels\": [],\n",
    "    }\n",
    "    num_samples_per_segment = int(samples_per_track / num_segments)\n",
    "    expected_num_mfcc_vectors_per_segment = ceil(\n",
    "        num_samples_per_segment / hop_length\n",
    "    )  # round up always\n",
    "\n",
    "    # Loop through all the data\n",
    "    for i, (dirpath, _, filenames) in enumerate(os.walk(dataset_path)):\n",
    "        # dirpath = current folder path\n",
    "        # dirnames = subfolders in dirpath\n",
    "        # filenames = all files in dirpath\n",
    "\n",
    "        # ensure that we're not at the root level (Audio folder)\n",
    "        if dirpath != str(dataset_path):\n",
    "            # save the semantic label\n",
    "            dirpath_components = dirpath.split(os.sep)\n",
    "            semantic_label = dirpath_components[-1]\n",
    "            # Subtract 1 to skip the root folder\n",
    "            semantic_to_numeric[semantic_label] = i - 1\n",
    "            print(f\"\\nProcessing {semantic_label}\")\n",
    "\n",
    "            # process files\n",
    "            for filename in filenames:\n",
    "                # load audio file\n",
    "                file_path = Path(dirpath, filename)  # os.path.join(dirpath, filename)\n",
    "                try:\n",
    "                    signal, sr = librosa.load(\n",
    "                        file_path  # , sr=SAMPLE_RATE, duration=DURATION\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_path}: {e}\")\n",
    "                    continue\n",
    "                # process segments extracting mfcc and storing data\n",
    "                for s in range(num_segments):\n",
    "                    start_sample = num_samples_per_segment * s\n",
    "                    finish_sample = start_sample + num_samples_per_segment\n",
    "\n",
    "                    mfcc = librosa.feature.mfcc(\n",
    "                        y=signal[start_sample:finish_sample],\n",
    "                        sr=sr,\n",
    "                        n_fft=n_fft,\n",
    "                        n_mfcc=n_mfcc,\n",
    "                        hop_length=hop_length,\n",
    "                    )\n",
    "                    mfcc = mfcc.T\n",
    "\n",
    "                    # store mfcc for segment if it has expected length\n",
    "                    if len(mfcc) == expected_num_mfcc_vectors_per_segment:\n",
    "                        # can't save numpy arrays as json files\n",
    "                        data[\"mfcc\"].append(mfcc.tolist())\n",
    "                        data[\"labels\"].append(i - 1)\n",
    "                        print(f\"{file_path}, segment:{s+1}\")\n",
    "    data[\"mappings\"] = semantic_to_numeric\n",
    "    with open(json_path, \"w\") as fp:\n",
    "        json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mfcc(genres_dir, \"data.json\", SAMPLES_PER_TRACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_path):\n",
    "    \"\"\"\n",
    "    Loads training dataset from json file.\n",
    "        :param data_path (str): Path to json file containing data\n",
    "        :return X (ndarray): Inputs\n",
    "        :return y (ndarray): Targets\n",
    "    \"\"\"\n",
    "\n",
    "    with open(dataset_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    # convert lists to numpy arrays\n",
    "    X = np.array(data[\"mfcc\"])\n",
    "    # X = np.array(data[\"spectrogram\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "    mappings = data[\"mappings\"]\n",
    "    return X, y, mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "class LSTMGenreModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_size)\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        # out shape: (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Take the final output and classify\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        # out shape: (batch, num_classes)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Dataset\n",
    "class MFCCDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfccs = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        return mfccs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, _ = load_data(\"data.json\")\n",
    "X = np.array(X, dtype=np.float32)\n",
    "y = torch.tensor(y, dtype=torch.long).clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(X, y, test_size, validation_size, shuffle=True, random_state=42):\n",
    "    # create train, validation and test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, shuffle=shuffle, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # create train/validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=validation_size\n",
    "    )\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_classes = 10  # number of genres\n",
    "input_size = 13  # number of MFCC coefficients\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, X_val, labels_val = prepare_datasets(X, y, 0.25, 0.2)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = MFCCDataset(X_train, y_train)\n",
    "test_dataset = MFCCDataset(X_test, y_test)\n",
    "val_dataset = MFCCDataset(X_val, labels_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train model\n",
    "model = LSTMGenreModel(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "sample_input = torch.zeros(1, 128, 13).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    for mfccs_test, labels_test in train_loader:\n",
    "        # Move tensors to CUDA\n",
    "        mfccs_test = mfccs_test.to(device)\n",
    "        labels_test = labels_test.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(mfccs_test)\n",
    "\n",
    "        loss = criterion(outputs, labels_test)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track training metrics\n",
    "        train_loss += loss.item()\n",
    "        # Calculate accuracy for current batch\n",
    "        batch_correct = (outputs.argmax(1) == labels_test).float().sum()\n",
    "        # Convert to python scalar for logging\n",
    "        train_correct += batch_correct.item()\n",
    "    # Average training metrics\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = train_correct / len(train_loader.dataset)  # type: ignore\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mfccs_val, labels_val in val_loader:\n",
    "            # Move tensors to CUDA\n",
    "            mfccs_val = mfccs_val.to(device)\n",
    "            labels_val = labels_val.to(device)\n",
    "            outputs = model(mfccs_val)\n",
    "            val_loss += criterion(outputs, labels_val).item()\n",
    "            val_correct += (outputs.argmax(1) == labels_val).sum().item()\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "    val_accuracy = val_correct / len(test_loader.dataset)  # type: ignore\n",
    "\n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/Valid\", val_loss, epoch)\n",
    "\n",
    "    writer.add_scalar(\"Accuracy/Train\", train_accuracy, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Valid\", val_accuracy, epoch)\n",
    "\n",
    "    # Log FC layer weights\n",
    "    writer.add_histogram(\"FC Weights\", model.fc.weight)\n",
    "\n",
    "    # Log RNN layer weights\n",
    "    writer.add_histogram(\"RNN Weights\", model.lstm.weight_ih_l0)\n",
    "\n",
    "    writer.add_graph(model, sample_input)\n",
    "\n",
    "model.to(\"cpu\")\n",
    "\n",
    "# Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for mfccs_test, labels_test in test_loader:\n",
    "        outputs = model(mfccs_test)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels_test.size(0)\n",
    "        correct += (predicted == labels_test).sum().item()\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log parameters\n",
    "mlflow.start_run()\n",
    "\n",
    "mlflow.log_metric(\"accuracy\", accuracy)\n",
    "mlflow.log_param(\"SAMPLE_RATE\", SAMPLE_RATE)\n",
    "mlflow.log_param(\"DURATION\", DURATION)\n",
    "mlflow.log_param(\"NUM_SEGMENTS\", NUM_SEGMENTS)\n",
    "mlflow.log_param(\"SAMPLES_PER_TRACK\", SAMPLES_PER_TRACK)\n",
    "mlflow.log_param(\"num_classes\", num_classes)\n",
    "mlflow.log_param(\"input_size\", input_size)\n",
    "mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "mlflow.log_param(\"num_layers\", num_layers)\n",
    "mlflow.log_param(\"batch_size\", batch_size)\n",
    "mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "mlflow.log_param(\n",
    "    \"data_version\",\n",
    "    subprocess.check_output(\n",
    "        [\"git\", \"rev-parse\", \"HEAD\"], universal_newlines=True\n",
    "    ).strip(),\n",
    ")\n",
    "\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Genre-Classification-Tl9UQQdU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
